{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac2f1428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[31373, 50257, 33847], [31373, 50257, 33847, 50257, 345]], 'attention_mask': [[1, 1, 1], [1, 1, 1, 1, 1]], 'speaker_ids': [[50258, 50258, 50259], [50258, 50258, 50259, 50259, 50258]]}\n",
      "hello,,,there;everybody.whats<ts>     how are you?<ts>\n",
      "hello there everybody whats<ts> how are you<ts>\n",
      "hello there how are you today<ts> i'm doing good thank you<ts> that's great<ts>\n",
      "[50258, 50258, 50258, 50258, 50258, 50258, 50258, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50258, 50258, 50258, 50258]\n",
      "1201\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Regex\n",
    "from tokenizers.normalizers import (\n",
    "    Lowercase,\n",
    "    NFD,\n",
    "    StripAccents,\n",
    "    Replace,\n",
    "    Strip,\n",
    "    Sequence,\n",
    ")\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "from typing import List, Union\n",
    "import torch\n",
    "import re\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "TS_TOKENS = {\n",
    "    \"eos_token\": \"<ts>\",\n",
    "    \"pad_token\": \"<|endoftext|>\",\n",
    "    \"additional_special_tokens\": [\"<speaker1>\", \"<speaker2>\",\"<speaker3>\"],\n",
    "}\n",
    "\n",
    "\n",
    "class SpokenNormalizer:\n",
    "    \"\"\"\n",
    "    Normalizer (as in the `tokenizers` framework) which removes punctuation, force lowercase, etc\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.normalizer = SpokenNormalizer.build_normalizer()\n",
    "\n",
    "    def normalize_string(self, s):\n",
    "        s = self.add_whitespace_after_punctuation(s)\n",
    "        return self.normalizer.normalize_str(s)\n",
    "\n",
    "    def add_whitespace_after_punctuation(self, s):\n",
    "        \"\"\"\n",
    "        Don't know how to do this with the `tokenizers` library.\n",
    "        So simple regexp for now...\n",
    "\n",
    "        Without this function:\n",
    "\n",
    "            \"hello,,,there;everybody.whats     how are you?\"\n",
    "            -> \"hellothereeverybodywhats how are you\" (once decoded)\n",
    "\n",
    "        With:\n",
    "\n",
    "            \"hello,,,there;everybody.whats     how are you?\"\n",
    "            -> \"hello there everybody whats how are you\"\n",
    "\n",
    "        \"\"\"\n",
    "        s = re.sub(r\"[\\,\\.\\:\\;]+(\\w+)\", r\" \\1\", s)\n",
    "        return s\n",
    "\n",
    "    @staticmethod\n",
    "    def build_normalizer():\n",
    "        normalizer = Sequence(\n",
    "            [\n",
    "                NFD(),\n",
    "                Lowercase(),\n",
    "                StripAccents(),\n",
    "                Replace(Regex(r'[\\.\\,\\!\\?\\:\\;\\)\\(\\[\\]\"\\-]'), \"\"),  # punctuation\n",
    "                Replace(Regex(r\"\\s\\s+\"), \" \"),  # double spaces\n",
    "                Strip(),\n",
    "            ]\n",
    "        )\n",
    "        return normalizer\n",
    "\n",
    "\n",
    "class SpokenDialogTokenizer(SpokenNormalizer):\n",
    "    \"\"\"\n",
    "    A tokenizer wrapper for `AutoTokenizer.from_pretrained` which cleans/normalizes text\n",
    "    strings, removes punctuations and creates `speaker_ids` (like TransferTransfo and similiar to Bert) where each utterance\n",
    "    is imbued with a token corresponding to the correct speaker (<speaker1> and <speaker2>).\n",
    "\n",
    "    Should work (kind of) like the normal `Tokenizers` in the `transformers` framework.\n",
    "\n",
    "    IMPORTANT!!!\n",
    "    ------------\n",
    "    Do not have spaces prior to `eos_token`/<ts> in the complete dialog strings.\n",
    "    The tokenizer inserts EMPTY SPACE!!!\n",
    "\n",
    "    'hello there <ts>' -> ['hello', 'Ġthere' 'Ġ' '<ts>']\n",
    "\n",
    "    this is bad!\n",
    "    -----------------------------\n",
    "\n",
    "    text_string = 'Yesterday Hello ther, \"honey\"<ts> godday... you are great<ts> Not as good as you!<ts>'\n",
    "    o = tokenizer(text_string, return_tensors=\"pt\")\n",
    "\n",
    "    ----------------------------------------------------\n",
    "\n",
    "    text_list = [\n",
    "        'Yesterday Hello ther, \"honey\"',\n",
    "        \"godday... you are great\",\n",
    "        \"Not as good as you!\",\n",
    "    ]\n",
    "    o2 = tok(text_list, return_tensors=\"pt\")\n",
    "    print(o2[\"speaker_ids\"] == o[\"speaker_ids\"])\n",
    "    for inps, spkrs in zip(o[\"input_ids\"], o[\"speaker_ids\"]):\n",
    "        for i, s in zip(inps, spkrs):\n",
    "            print(i.item(), s.item())\n",
    "\n",
    "    ----------------------------------------------------\n",
    "\n",
    "    list_of_lists = [text_list, text_list[:-1], text_list[:-2]]\n",
    "    o = tok(text_string)\n",
    "    o2 = tok(text_list)\n",
    "    print(o2[\"speaker_ids\"] == o[\"speaker_ids\"])\n",
    "    for i, s in zip(o[\"input_ids\"], o[\"speaker_ids\"]):\n",
    "        print(i, s)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    MODELS = [\n",
    "        \"microsoft/DialoGPT-small\",\n",
    "        \"microsoft/DialoGPT-medium\",\n",
    "        \"microsoft/DialoGPT-large\",\n",
    "        \"gpt2\",\n",
    "    ]\n",
    "\n",
    "    @property\n",
    "    def unk_token(self):\n",
    "        return self._tokenizer.unk_token\n",
    "\n",
    "    @property\n",
    "    def unk_token_id(self):\n",
    "        return self._tokenizer.unk_token_id\n",
    "\n",
    "    @property\n",
    "    def eos_token(self):\n",
    "        return self._tokenizer.eos_token\n",
    "\n",
    "    @property\n",
    "    def eos_token_id(self):\n",
    "        return self._tokenizer.eos_token_id\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained_model_name_or_path: str = \"gpt2\",\n",
    "        normalization=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.name_or_path = pretrained_model_name_or_path\n",
    "        if pretrained_model_name_or_path not in self.MODELS:\n",
    "            print(\n",
    "                f\"WARNING: not tested for {pretrained_model_name_or_path} tread carefully!\\n{self.MODELS}\"\n",
    "            )\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(\n",
    "            pretrained_model_name_or_path, max_model_input_sizes=None\n",
    "        )\n",
    "        self.normalization = normalization\n",
    "\n",
    "        # Set to large number to avoid warnings\n",
    "        # Manually keep track of your models maximum input length\n",
    "        self._tokenizer.model_max_length = 1e30\n",
    "\n",
    "        # This goes in logging\n",
    "        num_added_toks = self._tokenizer.add_special_tokens(TS_TOKENS)\n",
    "\n",
    "        s = \"Tokenizer initialization:\\n\"\n",
    "        s += f\"\\tWe added {num_added_toks} tokens -> Special token map\\n\"\n",
    "        for k, v in self._tokenizer.special_tokens_map.items():\n",
    "            s += f\"\\t{k}: {v}\\n\"\n",
    "        logger.info(s)\n",
    "\n",
    "        # Turn-shift Token (eos_token)\n",
    "        # self.eos_token = self._tokenizer.eos_token\n",
    "        # self.eos_token_id = self._tokenizer.eos_token_id\n",
    "        # self.unk_token = self._tokenizer.unk_token\n",
    "        # self.unk_token_id = self._tokenizer.unk_token_id\n",
    "\n",
    "        # Speaker Tokens\n",
    "        self.sp1_token = TS_TOKENS[\"additional_special_tokens\"][0]\n",
    "        self.sp2_token = TS_TOKENS[\"additional_special_tokens\"][1]\n",
    "        self.sp3_token = TS_TOKENS[\"additional_special_tokens\"][2]\n",
    "        self.sp1_token_id = self._tokenizer.convert_tokens_to_ids(self.sp1_token)\n",
    "        self.sp2_token_id = self._tokenizer.convert_tokens_to_ids(self.sp2_token)\n",
    "        self.sp3_token_id = self._tokenizer.convert_tokens_to_ids(self.sp3_token)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self._tokenizer.__repr__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._tokenizer)\n",
    "\n",
    "    def normalize(self, string: str) -> str:\n",
    "        if self.normalization:\n",
    "            return self.normalize_string(string)\n",
    "        return string\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        text: Union[str, List[str], List[List[str]]],\n",
    "        return_token_type_ids: bool = True,\n",
    "        include_pre_space: bool = False,\n",
    "        include_end_ts: bool = True,\n",
    "        **kwargs,\n",
    "    ) -> BatchEncoding:\n",
    "        \"\"\"\n",
    "        SpokenDialogTokenizer tokenization.\n",
    "\n",
    "        `text` can be either a String, a List of Strings, or a List of Lists of Strings. The behaviour of\n",
    "        this function depends on the `single_dialog` flag.\n",
    "\n",
    "        `text` is String:           representation of entire dialog (including eos_token)\n",
    "        `text` is List[str]:        representation of turns in a dialog (no eos_tokens)\n",
    "        `text` is List[List[str]]:  multiple dialogs (lists of strings) (no eos_tokens)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # List of lists\n",
    "        if isinstance(text, list) and isinstance(text[0], list):\n",
    "            ret = {}\n",
    "            for text_list in text:\n",
    "                o = self(\n",
    "                    text_list,\n",
    "                    include_pre_space=include_pre_space,\n",
    "                    include_end_ts=include_end_ts,\n",
    "                )\n",
    "\n",
    "                for k, v in o.items():\n",
    "                    if not k in ret:\n",
    "                        ret[k] = []\n",
    "                    ret[k].append(v)\n",
    "            return ret\n",
    "\n",
    "        # List of strings, a dialog: ['hello', 'hello to you']\n",
    "        elif isinstance(text, List):\n",
    "            dialog_string = \"\"\n",
    "            if include_pre_space:\n",
    "                dialog_string = \" \"\n",
    "            dialog_string += self.normalize(text[0])\n",
    "            if len(text) > 1:\n",
    "                dialog_string += self.eos_token\n",
    "                for text_string in text[1:-1]:\n",
    "                    dialog_string += \" \" + self.normalize(text_string) + self.eos_token\n",
    "                dialog_string += \" \" + self.normalize(text[-1])\n",
    "            if include_end_ts:\n",
    "                dialog_string += self.eos_token\n",
    "            text = dialog_string\n",
    "        else:\n",
    "            text = self.normalize(text)\n",
    "\n",
    "        encoding = self._tokenizer(\n",
    "            text=text,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        if return_token_type_ids:\n",
    "            encoding[\"speaker_ids\"] = self._extract_speaker_states(\n",
    "                encoding[\"input_ids\"]\n",
    "            )\n",
    "        return encoding\n",
    "\n",
    "    def _extract_speaker_states(self, input_ids):\n",
    "        # extract speaker states\n",
    "        back_to_list = False\n",
    "        if not isinstance(input_ids, torch.Tensor):\n",
    "            input_ids = torch.tensor(input_ids).unsqueeze(0)  # with batch dim\n",
    "            back_to_list = True\n",
    "        # initialize with speaker 1\n",
    "        speaker_ids = torch.ones_like(input_ids) * self.sp1_token_id\n",
    "        batch, eos_idx = torch.where(input_ids == self.eos_token_id)\n",
    "        for b in batch.unique():\n",
    "            tmp_eos = eos_idx[batch == b]\n",
    "            if len(tmp_eos) == 1:\n",
    "                speaker_ids[b, eos_idx + 1 :] = self.sp2_token_id\n",
    "            else:\n",
    "                start = tmp_eos[0]\n",
    "                for i, eos in enumerate(tmp_eos[1:]):\n",
    "                    if i % 2 == 0:\n",
    "                        sp = self.sp2_token_id\n",
    "                        speaker_ids[b, start + 1 : eos + 1] = sp\n",
    "                    start = eos\n",
    "                if i % 2 == 1:  # add sp2 tokens after last eos if i is odd\n",
    "                    speaker_ids[b, start + 1 :] = self.sp2_token_id\n",
    "\n",
    "        if back_to_list:\n",
    "            speaker_ids = speaker_ids.squeeze().tolist()\n",
    "            if isinstance(speaker_ids, int):\n",
    "                speaker_ids = [speaker_ids]\n",
    "\n",
    "        return speaker_ids\n",
    "\n",
    "    def idx_to_tokens(self, ids):\n",
    "        def list_ids_to_string(ids):\n",
    "            return [\n",
    "                self.convert_tokens_to_string(t)\n",
    "                for t in self.convert_ids_to_tokens(ids)\n",
    "            ]\n",
    "\n",
    "        # tokenize keep tokens\n",
    "        if isinstance(ids, torch.Tensor):\n",
    "            ids = ids.tolist()\n",
    "\n",
    "        if isinstance(ids, list):\n",
    "            if isinstance(ids[0], list):\n",
    "                ret = [list_ids_to_string(ids_list) for ids_list in ids]\n",
    "            else:\n",
    "                ret = list_ids_to_string(ids)\n",
    "        else:\n",
    "            ret = self.convert_tokens_to_string(self.convert_ids_to_tokens(ids))\n",
    "        return ret\n",
    "\n",
    "    def pad(self, *args, **kwargs):\n",
    "        return self._tokenizer.pad(*args, **kwargs)\n",
    "\n",
    "    def decode(self, *args, **kwargs):\n",
    "        return self._tokenizer.decode(*args, **kwargs)\n",
    "\n",
    "    def convert_ids_to_tokens(self, *args, **kwargs):\n",
    "        return self._tokenizer.convert_ids_to_tokens(*args, **kwargs)\n",
    "\n",
    "    def convert_tokens_to_ids(self, *args, **kwargs):\n",
    "        return self._tokenizer.convert_tokens_to_ids(*args, **kwargs)\n",
    "\n",
    "    def convert_tokens_to_string(self, *args, **kwargs):\n",
    "        return self._tokenizer.convert_tokens_to_string(*args, **kwargs).strip()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    pretrained_model_name_or_path = \"gpt2\"\n",
    "    tokenizer = SpokenDialogTokenizer(pretrained_model_name_or_path)\n",
    "\n",
    "    turn_list = [\"hello there how are you today?\"]\n",
    "    turn_list = [\"hello\", \"good\"]\n",
    "    # turn_list = [\"hello there how are you today?\", \"good\", \"great\"]\n",
    "    # turn_list = [\"hello there how are you today?\", \"good\", \"great\", 'yes']\n",
    "    # turn_list = [\"hello there how are you today?\", \"good\", \"great\", 'yes', 'hello']\n",
    "    # turn_list = [\"hello there how are you today?\", \"good\", \"great\", 'yes', 'hello', 'there']\n",
    "    out = tokenizer([[\"hello\", \"bye\"], [\"hello\", \"bye\", \"you\"]], include_end_ts=False)\n",
    "    print(out)\n",
    "\n",
    "    # double spaces\n",
    "    s = \"hello,,,there;everybody.whats<ts>     how are you?<ts>\"\n",
    "    print(s)\n",
    "    t = tokenizer(s)\n",
    "    print(tokenizer.decode(t[\"input_ids\"]))\n",
    "\n",
    "    s = \"Hello there, how are you today?<ts> I'm doing good thank you!<ts> That's great<ts>\"\n",
    "    outputs = tokenizer(s)\n",
    "\n",
    "    print(tokenizer.decode(outputs[\"input_ids\"]))\n",
    "    print(outputs[\"speaker_ids\"])\n",
    "\n",
    "    #outputs[\"speaker\"]\n",
    "\n",
    "    turn_list = [\n",
    "        \"hello there how are you doing today?\",\n",
    "        \"I'm doing very well thank you, how about you?\",\n",
    "        \"well, I'm sad\",\n",
    "    ]\n",
    "\n",
    "    i = tokenizer(turn_list, include_end_ts=False, include_pre_space=True)[\"input_ids\"]\n",
    "    d = tokenizer.decode(i)\n",
    "\n",
    "    very_long_string = \"\"\n",
    "    for i in range(150):\n",
    "        very_long_string += \"I'm doing very well thank you, how about you?\"\n",
    "    print(len(very_long_string.split(\" \")))\n",
    "\n",
    "    _ = tokenizer(very_long_string)\n",
    "\n",
    "    turn_list = [\n",
    "        \"hello there how are you doing today?\",\n",
    "        \"I'm doing very well thank you, how about you?\",\n",
    "        \"well, I'm sad\",\n",
    "    ]\n",
    "    tok_out = tokenizer(turn_list, include_end_ts=False)\n",
    "    #ids_list = tok_out[\"input_ids\"]\n",
    "    #ids_list = tok_out[\"input_ids\"]\n",
    "    #ids_tens = torch.tensor(tok_out[\"input_ids\"])\n",
    "    #t1 = tokenizer.idx_to_tokens(ids_list)\n",
    "    #t2 = tokenizer.idx_to_tokens(ids_tens)\n",
    "    #t3 = tokenizer.idx_to_tokens(ids_list[0])\n",
    "\n",
    "    #outputs = tokenizer(list_of_lists, include_end_ts=False)\n",
    "\n",
    "    output_strings = []\n",
    "    for out in outputs[\"input_ids\"]:\n",
    "        output_strings.append(tokenizer.decode(out))\n",
    "\n",
    "    #assert output_strings == output_list_of_lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea0706cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     me003|i guess.|S|s|s\n",
      "0                              me011|okay we're on.|S|s|s\n",
      "1       me011|so just make sure that your wireless mik...\n",
      "2                                  me003|check one.|S|s|s\n",
      "3                                  me003|check one.|S|s|s\n",
      "4       me011|and you should be able to see which one ...\n",
      "...                                                   ...\n",
      "108196                                me011|this is|D|s|s\n",
      "108197                   fe016|it's really helpful.|S|s|s\n",
      "108198  fe016|i mean adam and don will sort of meet.|S...\n",
      "108199             fe016|and i think that's great.|S|s|ba\n",
      "108200                          fe016|very useful.|S|s|ba\n",
      "\n",
      "[108201 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "df=pd.read_csv(\"Transcriptions.csv\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3dc2b191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Dialogue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bob</td>\n",
       "      <td>Hi Alan, third time today.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alan</td>\n",
       "      <td>Yeah, yeah, yeah, it’s a busy clinic…</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bob</td>\n",
       "      <td>[Inaudible 00:03].  Okay.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alan</td>\n",
       "      <td>…busy clinic.  So we’ve got Kate, she’s a midd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bob</td>\n",
       "      <td>I don’t…is she known to us already?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Speaker                                           Dialogue\n",
       "0     Bob                         Hi Alan, third time today.\n",
       "1    Alan              Yeah, yeah, yeah, it’s a busy clinic…\n",
       "2     Bob                          [Inaudible 00:03].  Okay.\n",
       "3    Alan  …busy clinic.  So we’ve got Kate, she’s a midd...\n",
       "4     Bob                I don’t…is she known to us already?"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d899ca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lists = []\n",
    "for text in df['Dialogue']:\n",
    "    text_lists.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c56fcf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name_or_path=\"microsoft/DialoGPT-small\"\n",
    "tokenizer = SpokenDialogTokenizer(pretrained_model_name_or_path)\n",
    "\n",
    "# tokenizer.eos_token: '<ts>'\n",
    "# tokenizer.eos_token_id: 50257\n",
    "\n",
    "# tokenizer.sp1_token: '<speaker1>'\n",
    "# tokenizer.sp1_token_id: 50258\n",
    "\n",
    "# tokenizer.sp2_token: '<speaker2>'\n",
    "# tokenizer.sp2_token_id: 50259\n",
    "\n",
    "\n",
    "\n",
    "outputs = tokenizer(text_lists)\n",
    "\n",
    "# print(outputs.keys())\n",
    "# >>> dict_keys(['input_ids', 'attention_mask', 'speaker_ids'])\n",
    "\n",
    "# input_ids: word embedding indices\n",
    "# >>> input_ids: [8505, ...,  220, 50257, 5770, ..., 50257]\n",
    "\n",
    "# attention_mask: mask to omit `pad_token` in loss\n",
    "# >>> attention_mask: [1, ...,  1, 1, 1, ..., 1]\n",
    "\n",
    "# speaker_ids: dialog state embeddings corresponind to speaker id (binary)\n",
    "# >>> speaker_ids: [50258, ..., 50259, ..., 50258]\n",
    "\n",
    "decoded_input = tokenizer.decode(outputs['input_ids']) # arugment must be a list\n",
    "\n",
    "# >>> 'yesterday hello ther honey <ts> godday you are great <ts> not as good as you <ts>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1ac5aa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=outputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "843e4630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 [==============================] - 51s 359ms/step - loss: 0.4013 - accuracy: 0.9435\n",
      "29/29 [==============================] - 6s 122ms/step - loss: 0.2326 - accuracy: 0.9396\n",
      "Test Loss: 0.2325638085603714\n",
      "Test Accuracy: 0.9396268129348755\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sequences = []\n",
    "labels = []\n",
    "seq = []\n",
    "for token in data:\n",
    "    seq.append(token)\n",
    "    if token == 50257:  # Check for the turn-shift token\n",
    "        labels.pop(0)\n",
    "        labels.append(1)  # Append 1 to indicate the presence of a turn-shift token\n",
    "        #sequences.append(seq.copy())  # Append the sequence\n",
    "        seq = []  # Reset the sequence\n",
    "    else:\n",
    "        labels.append(0)  # Append 0 to indicate no turn-shift token\n",
    "        sequences.append(seq.copy()) \n",
    "## Pad sequences to make them of equal length\n",
    "padded_sequences = pad_sequences(sequences, padding='post')\n",
    "\n",
    "# Convert sequences and labels to numpy arrays\n",
    "sequences_np = np.array(padded_sequences)\n",
    "labels_np = np.array(labels)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(sequences_np, labels_np, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape X_train and X_test to match the expected input shape of LSTM\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Define the optimizer with learning rate schedule\n",
    "initial_learning_rate = 6.25e-5\n",
    "lr_schedule = ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=10000, decay_rate=0.96, staircase=True\n",
    ")\n",
    "optimizer = AdamW(learning_rate=lr_schedule)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=1, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
